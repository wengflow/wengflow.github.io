<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Weng Fei Low 刘永辉</title> <meta name="author" content="Weng Fei Low"/> <meta name="description" content="Personal website of Low Weng Fei "/> <meta property="og:site_name" content="Weng Fei Low 刘永辉"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Weng Fei Low 刘永辉 | about"/> <meta property="og:url" content="https://wengflow.github.io/"/> <meta property="og:description" content="Personal website of Low Weng Fei "/> <meta property="og:image" content="/assets/img/prof_pic.jpg"/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="about"/> <meta name="twitter:description" content="Personal website of Low Weng Fei "/> <meta name="twitter:image" content="/assets/img/prof_pic.jpg"/> <meta name="twitter:site" content="@wengflow"/> <meta name="twitter:creator" content="@wengflow"/> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Weng Fei  Low"
        },
        "url": "https://wengflow.github.io/",
        "@type": "WebSite",
        "description": "Personal website of Low Weng Fei
",
        "headline": "about",
        "sameAs": ["https://scholar.google.com/citations?user=JNO9kRsAAAAJ", "https://github.com/wengflow", "https://www.linkedin.com/in/wengfei-low", "https://twitter.com/wengflow"],
        "name": "Weng Fei  Low",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="apple-touch-icon" sizes="180x180" href="/assets/img/icon/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icon/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icon/favicon-16x16.png"> <link rel="manifest" href="/assets/img/icon/site.webmanifest"> <link rel="shortcut icon" href="/assets/img/icon/favicon.ico"> <meta name="msapplication-TileColor" content="#ffc40d"> <meta name="msapplication-config" content="/assets/img/icon/browserconfig.xml"> <meta name="theme-color" content="#ffffff"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wengflow.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4WDKH4YC37"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4WDKH4YC37");</script> </head> <body class=" "> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="same-line"><span class="font-weight-bold">Weng Fei</span> Low</span> ｜ <span class="same-line">刘<span class="font-weight-bold">永辉</span></span> </h1> <p class="desc"></p> </header> <br> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Hi <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20">! I’m from Kuala Lumpur, Malaysia <img class="emoji" title=":malaysia:" alt=":malaysia:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f2-1f1fe.png" height="20" width="20">.</p> <p>I recently graduated with a PhD <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20"> under the supervision of <a href="https://www.comp.nus.edu.sg/~leegh/" target="_blank" rel="noopener noreferrer">Associate Professor Gim Hee Lee</a> at the <a href="http://ids.nus.edu.sg" target="_blank" rel="noopener noreferrer">Institute of Data Science, National University of Singapore (NUS)</a>. I previously obtained my Bachelor of Engineering (Honors) in Computer Engineering also from NUS in 2020.</p> <p>My research interest lies in 3D/4D computer vision, with a particular focus on <strong>3D/4D scene representation, reconstruction, and generation</strong>. I’m also passionate about multimodal learning and robotics, having worked with diverse sensing modalities including RGB-D cameras, event cameras, IMUs, phased array mmWave radars and acoustic sensors.</p> <p><strong>I’m currently in the job market</strong> <img class="emoji" title=":mag_right:" alt=":mag_right:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png" height="20" width="20">. Do reach out if you have any opportunities that you think I might be a good fit for!</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 35vh"> <table class="table table-sm table-borderless"> <tr> <th scope="row">May 26, 2025</th> <td> I <strong>successfully defended my PhD thesis</strong> titled “Neural 3D Scene Representation and Reconstruction” </td> </tr> <tr> <th scope="row">Feb 7, 2025</th> <td> I received <strong>my second Research Achievement Award</strong> from NUS School of Computing </td> </tr> <tr> <th scope="row">Dec 2, 2024</th> <td> I started my <strong>research scientist internship</strong> at Meta Reality Labs, Zurich </td> </tr> <tr> <th scope="row">Jul 1, 2024</th> <td> A paper on <a href="/#low2024_deblur-e-nerf">NeRF recontruction from motion-blurred events</a> accepted at ECCV 2024 </td> </tr> <tr> <th scope="row">Jan 9, 2024</th> <td> I received the <strong>Research Achievement Award</strong> from NUS School of Computing </td> </tr> <tr> <th scope="row">Jul 18, 2023</th> <td> A paper on <a href="/#low2023_robust-e-nerf">robust NeRF recontruction with moving event cameras</a> accepted at ICCV 2023 </td> </tr> <tr> <th scope="row">Jan 31, 2023</th> <td> A US patent on <a href="/#leenayongwut2023_ssl">sound source classification and localization</a> granted </td> </tr> <tr> <th scope="row">Dec 2, 2022</th> <td> I received the <strong>Student Research Achievement Award</strong> from NUS IDS </td> </tr> <tr> <th scope="row">Jul 4, 2022</th> <td> A paper on <a href="/#low2022_mna">explicit neural surface representation</a> accepted at ECCV 2022 </td> </tr> </table> </div> </div> <div class="publications"> <h2>publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/deblur-e-nerf.png"></div> <div id="low2024_deblur-e-nerf" class="col-sm-8"> <div class="title">Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions</div> <div class="author"> <em>Weng Fei Low</em>, and <a href="https://www.comp.nus.edu.sg/~leegh/" target="_blank" rel="noopener noreferrer">Gim Hee Lee</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wengflow.github.io/deblur-e-nerf" class="btn btn-sm z-depth-0" role="button">Website</a> <a href="http://arxiv.org/abs/2409.17988" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/wengflow/deblur-e-nerf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://github.com/wengflow/rpg_esim" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Simulator</a> <a href="https://huggingface.co/datasets/wengflow/deblur-e-nerf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Dataset</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The distinctive design philosophy of event cameras makes them ideal for high-speed, high dynamic range &amp; low-light environments, where standard cameras underperform. However, event cameras also suffer from motion blur, especially under these challenging conditions, contrary to what most think. This is due to the limited bandwidth of the event sensor pixel, which is mostly proportional to the light intensity. Thus, to ensure event cameras can truly excel in such conditions where it has an edge over standard cameras, event motion blur must be accounted for in downstream tasks, especially reconstruction. However, no prior work on reconstructing Neural Radiance Fields (NeRFs) from events, nor event simulators, have considered the full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a novel method to directly and effectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed or low-light conditions. The core component of this work is a physically-accurate pixel bandwidth model that accounts for event motion blur. We also introduce a threshold-normalized total variation loss to better regularize large textureless patches. Experiments on real &amp; novel realistically simulated sequences verify our effectiveness. Our code, event simulator and synthetic event dataset are open-sourced.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">low2024_deblur-e-nerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Low, Weng Fei and Lee, Gim Hee}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/anti-aliased-gaussian.gif"></div> <div id="yan2023_anti-aliased-gaussian" class="col-sm-8"> <div class="title">Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering</div> <div class="author"> Zhiwen Yan,  <em>Weng Fei Low</em>, Yu Chen, and <a href="https://www.comp.nus.edu.sg/~leegh/" target="_blank" rel="noopener noreferrer">Gim Hee Lee</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://jokeryan.github.io/projects/ms-gs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="http://arxiv.org/abs/2311.17089" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13%-66% PSNR and 160%-2400% rendering speed improvement at 4x-128x scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splatting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yan2023_anti-aliased-gaussian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Zhiwen and Low, Weng Fei and Chen, Yu and Lee, Gim Hee}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/robust-e-nerf.gif"></div> <div id="low2023_robust-e-nerf" class="col-sm-8"> <div class="title">Robust e-NeRF: NeRF from Sparse &amp; Noisy Events under Non-Uniform Motion</div> <div class="author"> <em>Weng Fei Low</em>, and <a href="https://www.comp.nus.edu.sg/~leegh/" target="_blank" rel="noopener noreferrer">Gim Hee Lee</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wengflow.github.io/robust-e-nerf" class="btn btn-sm z-depth-0" role="button">Website</a> <a href="http://arxiv.org/abs/2309.08596" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/wengflow/robust-e-nerf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://github.com/wengflow/rpg_esim" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Simulator</a> <a href="https://huggingface.co/datasets/wengflow/robust-e-nerf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Dataset</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Event cameras offer many advantages over standard cameras due to their distinctive principle of operation: low power, low latency, high temporal resolution and high dynamic range. Nonetheless, the success of many downstream visual applications also hinges on an efficient and effective scene representation, where Neural Radiance Field (NeRF) is seen as the leading candidate. Such promise and potential of event cameras and NeRF inspired recent works to investigate on the reconstruction of NeRF from moving event cameras. However, these works are mainly limited in terms of the dependence on dense and low-noise event streams, as well as generalization to arbitrary contrast threshold values and camera speed profiles. In this work, we propose Robust e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving event cameras under various real-world conditions, especially from sparse and noisy events generated under non-uniform motion. It consists of two key components: a realistic event generation model that accounts for various intrinsic parameters (e.g. time-independent, asymmetric threshold and refractory period) and non-idealities (e.g. pixel-to-pixel threshold variation), as well as a complementary pair of normalized reconstruction losses that can effectively generalize to arbitrary speed profiles and intrinsic parameter values without such prior knowledge. Experiments on real and novel realistically simulated sequences verify our effectiveness. Our code, synthetic dataset and improved event simulator are public.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">low2023_robust-e-nerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust e-NeRF: NeRF from Sparse &amp; Noisy Events under Non-Uniform Motion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Low, Weng Fei and Lee, Gim Hee}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/mna.jpg"></div> <div id="low2022_mna" class="col-sm-8"> <div class="title">Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion</div> <div class="author"> <em>Weng Fei Low</em>, and <a href="https://www.comp.nus.edu.sg/~leegh/" target="_blank" rel="noopener noreferrer">Gim Hee Lee</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.14782" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/wengflow/minimal-neural-atlas" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Explicit neural surface representations allow for exact and efficient extraction of the encoded surface at arbitrary precision, as well as analytic derivation of differential geometric properties such as surface normal and curvature. Such desirable properties, which are absent in its implicit counterpart, makes it ideal for various applications in computer vision, graphics and robotics. However, SOTA works are limited in terms of the topology it can effectively describe, distortion it introduces to reconstruct complex surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a novel atlas-based explicit neural surface representation. At its core is a fully learnable parametric domain, given by an implicit probabilistic occupancy field defined on an open square of the parametric space. In contrast, prior works generally predefine the parametric domain. The added flexibility enables charts to admit arbitrary topology and boundary. Thus, our representation can learn a minimal atlas of 3 charts with distortion-minimal parameterization for surfaces of arbitrary topology, including closed and open surfaces with arbitrary connected components. Our experiments support the hypotheses and show that our reconstructions are more accurate in terms of the overall geometry, due to the separation of concerns on topology and geometry.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">low2022_mna</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Low, Weng Fei and Lee, Gim Hee}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/sofea.jpg"></div> <div id="low2020_sofea" class="col-sm-8"> <div class="title">SOFEA: A Non-Iterative and Robust Optical Flow Estimation Algorithm for Dynamic Vision Sensors</div> <div class="author"> <em>Weng Fei Low</em>, Zhi Gao, <a href="https://cde.nus.edu.sg/ece/staff/xiang-cheng/" target="_blank" rel="noopener noreferrer">Cheng Xiang</a>, and <a href="https://sites.google.com/view/bharath-ramesh/" target="_blank" rel="noopener noreferrer">Bharath Ramesh</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w6/Low_SOFEA_A_Non-Iterative_and_Robust_Optical_Flow_Estimation_Algorithm_for_CVPRW_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://openaccess.thecvf.com/content_CVPRW_2020/supplemental/Low_SOFEA_A_Non-Iterative_CVPRW_2020_supplemental.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> <a href="https://bitbucket.org/wengfei_nus/sofea" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>We introduce the single-shot optical flow estimation algorithm (SOFEA) to non-iteratively compute the continuous-time flow information of events produced from bio-inspired cameras such as the dynamic vision sensor (DVS). The output of a DVS is a stream of asynchronous spikes ("events"), transmitted at very minimal latency (1-10 ms), caused by local brightness changes. Due to this unconventional output, a continuous representation of events over time is invaluable to most applications using the DVS. To this end, SOFEA consolidates the spatio-temporal information on the surface of active events for flow estimation in a single-shot manner, as opposed to iterative methods in the literature. In contrast to previous works, this is also the first principled method towards finding locally optimal set of neighboring events for plane fitting using an adaptation of Prim’s algorithm. Consequently, SOFEA produces flow estimates that are more accurate across a wide variety of scenes compared to state-of-the-art methods. A direct application of such flow estimation is rendering sharp event images using the set of active events at a given time, which is further demonstrated and compared to existing works.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">low2020_sofea</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SOFEA: A Non-Iterative and Robust Optical Flow Estimation Algorithm for Dynamic Vision Sensors}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Low, Weng Fei and Gao, Zhi and Xiang, Cheng and Ramesh, Bharath}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="publications"> <h2>patents</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ssl_patent.jpg"></div> <div id="leenayongwut2023_ssl" class="col-sm-8"> <div class="title">Using Classified Sounds and Localized Sound Sources to Operate an Autonomous Vehicle</div> <div class="author"> Metarsit Leenayongwut, and <em>Weng Fei Low</em> </div> <div class="periodical"> <em>US Patent 11,567,510 Granted in</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://patents.google.com/patent/US11567510B2/en" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://patentimages.storage.googleapis.com/f4/f1/b0/b152e8f5983e19/US11567510.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>An ambient sound environment is captured by a microphone array of an autonomous vehicle traveling in the ambient sound environment. A perception module of the autonomous vehicle classifies sounds and localizes sound sources in the ambient sound environment. Classification is performed using spectrum analysis and/or machine learning. In an embodiment, sound sources within a field of view (FOV) of an image sensor of the autonomous vehicle are localized in a visual scene generated by the perception module. In an embodiment, one or more sound sources outside the FOV of the image sensors are localized in a static digital map. Localization is performed using parametric or non-parametric techniques and/or machine learning. The output of the perception module is input into a planning module of the autonomous vehicle to plan a route or trajectory for the autonomous vehicle in the ambient sound environment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">leenayongwut2023_ssl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using Classified Sounds and Localized Sound Sources to Operate an Autonomous Vehicle}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Leenayongwut, Metarsit and Low, Weng Fei}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{US Patent 11,567,510}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="inline_list"> <h2>experience</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th> Research Scientist Intern — Meta Reality Labs, Zurich </th> <td> Dec 2024 - Mar 2025 </td> </tr> <tr> <th> Autonomous Vehicle Intern — nuTonomy (now Motional), Singapore </th> <td> Jun 2018 - Dec 2018 </td> </tr> </table> </div> </div> <div class="inline_list"> <h2>teaching assistants</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th> CS5340: Uncertainty Modelling in AI </th> <td> AY2022/23 Semester 1 </td> </tr> <tr> <th> CS4277/CS5477: 3D Computer Vision </th> <td> AY2021/22 Semester 2 </td> </tr> </table> </div> </div> <div class="inline_list"> <h2>projects</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th> Smart Pet Collar for Early Symptom Detection — National GRIP Run 2 </th> <td> Jul 2025 - Sep 2025 </td> </tr> <tr> <th> Bumblebee Autonomous Systems — Autonomous Underwater and Surface Vehicles </th> <td> Sep 2017 - May 2020 </td> </tr> </table> </div> </div> <div class="inline_list"> <h2>awards</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th> NUS School of Computing Research Achievement Award </th> <td> Jan 2024, Feb 2025 </td> </tr> <tr> <th> NUS IDS Student Research Achievement Award </th> <td> Dec 2022 </td> </tr> <tr> <th> NUS PhD (Data Science) Scholarship </th> <td> Oct 2020 </td> </tr> <tr> <th> IEEE Singapore Computer Society Prize (Best Bachelor's Dissertation) </th> <td> Jun 2020 </td> </tr> <tr> <th> Maritime RobotX Challenge — 1st Place </th> <td> Dec 2018 </td> </tr> <tr> <th> RoboSub Competition — 2nd Place </th> <td> Aug 2018 </td> </tr> <tr> <th> NUS Dean's List </th> <td> 2018 - 2020 </td> </tr> <tr> <th> ASEAN Undergraduate Scholarship </th> <td> Mar 2016 </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="javascript:window.location.href='mailto:'+atob('d2VuZ2ZlaUB1Lm51cy5lZHU=')" rel="nofollow"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=JNO9kRsAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/wengflow" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/wengfei-low" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/wengflow" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Weng Fei Low. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Icon by <a href="https://www.flaticon.com/authors/freepik" target="_blank" rel="noopener noreferrer">Freepik</a> from <a href="https://www.flaticon.com" target="_blank" rel="noopener noreferrer">Flaticon</a>. Last updated: January 28, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>